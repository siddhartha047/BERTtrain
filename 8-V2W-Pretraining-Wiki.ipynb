{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  20\n"
     ]
    }
   ],
   "source": [
    "import torch# If there's a GPU available...\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" ##I will find a way to fix this later :(\n",
    "\n",
    "NUM_GPUS=0\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():  \n",
    "        device = torch.device(\"cuda\")\n",
    "        NUM_GPUS=torch.cuda.device_count()\n",
    "        print('There are %d GPU(s) available.' % NUM_GPUS)\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name())# If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")  \n",
    "except:\n",
    "    print('Cuda error using CPU instead.')\n",
    "    device = torch.device(\"cpu\")  \n",
    "    \n",
    "print(device)\n",
    "\n",
    "# device = torch.device(\"cpu\")  \n",
    "# print(device)\n",
    "\n",
    "NUM_PROCESSORS=multiprocessing.cpu_count()\n",
    "print(\"Cpu count: \",NUM_PROCESSORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading directory:  /scratch/gilbreth/das90/Dataset\n",
      "Model Saving directory: /scratch/gilbreth/das90/Dataset/NVD/Model/\n"
     ]
    }
   ],
   "source": [
    "#from ipynb.fs.full.Dataset import getDataset, getDummyDataset, Data        \n",
    "\n",
    "if os.uname()[1].find('gilbreth')==0: ##if not darwin(mac/locallaptop)\n",
    "    DIR='/scratch/gilbreth/das90/Dataset'\n",
    "elif os.uname()[1].find('unimodular')==0:\n",
    "    DIR='/scratch2/das90/Dataset'\n",
    "elif os.uname()[1].find('Siddharthas')==0:\n",
    "    DIR='/Users/siddharthashankardas/Purdue/Dataset'  \n",
    "else:\n",
    "    DIR='./Results'\n",
    "    \n",
    "from pathlib import Path\n",
    "Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_SAVE_DIR=DIR+'/NVD/Model/'\n",
    "\n",
    "Path(MODEL_SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Data loading directory: \", DIR)\n",
    "print(\"Model Saving directory:\", MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import zipfile\n",
    "import wget\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import RandomSampler,SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For reproduciblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "from random import sample\n",
    "\n",
    "seed_val = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "pl.seed_everything(seed_val)\n",
    "\n",
    "try:\n",
    "    torch.cuda.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except:\n",
    "    print(\"nothing to set for cudnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        # a very useful feature of pytorch lightning  which leads to the named variables that are passed in\n",
    "        # being available as self.hparams.<variable_name> We use this when refering to eg\n",
    "        # self.hparams.learning_rate\n",
    "\n",
    "        # freeze\n",
    "        self._frozen = False\n",
    "\n",
    "        # eg https://github.com/stefan-it/turkish-bert/issues/5\n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,\n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "\n",
    "        #print(config)\n",
    "\n",
    "        A = AutoModelForMaskedLM\n",
    "        self.model = A.from_pretrained(self.hparams.pretrained, config=config)\n",
    "\n",
    "        print('Model: ', type(self.model))\n",
    "        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        # there are some choices, as to how you can define the input to the forward function I prefer it this\n",
    "        # way, where the batch contains the input_ids, the input_put_mask and sometimes the labels (for\n",
    "        # training)\n",
    "        \n",
    "        #print(batch['input_ids'].shape)\n",
    "        #print(batch['labels'].shape)\n",
    "                \n",
    "        outputs = self.model(input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        labels=batch['labels'])\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n",
    "        # class stipulates you to overwrite. This we do here, by virtue of this definition\n",
    "        \n",
    "        # self refers to the model, which in turn acceses the forward method\n",
    "        loss = self(batch)\n",
    "        \n",
    "        #tensorboard_logs = {'train_loss': loss}\n",
    "        # pytorch lightning allows you to use various logging facilities, eg tensorboard with tensorboard we\n",
    "        # can track and easily visualise the progress of training. In this case\n",
    "        \n",
    "        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        #self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        #return {'loss': loss, 'log': tensorboard_logs}\n",
    "        # the training_step method expects a dictionary, which should at least contain the loss\n",
    "        return loss\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         val_loss = self(batch)\n",
    "#         self.log('val_loss', val_loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "#         return val_loss\n",
    "        \n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         test_loss = self(batch)\n",
    "#         self.log('test_loss', test_loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "#         return test_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # The configure_optimizers is a (virtual) method, specified in the interface, that the\n",
    "        # pl.LightningModule class wants you to overwrite.\n",
    "        # In this case we define that some parameters are optimized in a different way than others. In\n",
    "        # particular we single out parameters that have 'bias', 'LayerNorm.weight' in their names. For those\n",
    "        # we do not use an optimization technique called weight decay.\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "        optimizer_grouped_parameters = [{\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.01\n",
    "        }, {\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.0\n",
    "        }]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.learning_rate,\n",
    "                          eps=1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                          )\n",
    "\n",
    "        \n",
    "        # We also use a scheduler that is supplied by transformers.\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0, # Default value in run_glue.py\n",
    "            num_training_steps=self.hparams.num_training_steps)\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def getWiki(DIR):\n",
    "    \n",
    "    cache_dir=DIR\n",
    "    \n",
    "    dataset=load_dataset('wikipedia', '20200501.en', beam_runner='DirectRunner', cache_dir=cache_dir)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "class WikiDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, berttokenizer, collator, dataset):\n",
    "        self.tokenizer = berttokenizer\n",
    "        self.dataset = dataset\n",
    "        self.collator = collator\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.tokenizer(self.dataset['train'][idx]['text'], truncation=True, padding='max_length')  \n",
    "        item = self.collator([item])\n",
    "        item = {key: val[0] for key, val in item.items()}\n",
    "    \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows['train']\n",
    "        #return 2000\n",
    "        \n",
    "# A=AutoTokenizer\n",
    "# berttokenizer=A.from_pretrained('bert-base-uncased')\n",
    "# datacollator=DataCollatorForLanguageModeling(tokenizer=berttokenizer,mlm_probability=0.15, mlm=True)\n",
    "# wikidataset = WikiDataset(berttokenizer,datacollator, dataset)\n",
    "# wikidataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing(pl.LightningDataModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "#         self.save_hyperparameters()\n",
    "        if isinstance(args, tuple): args = args[0]\n",
    "        self.hparams = args\n",
    "        self.batch_size=self.hparams.batch_size        \n",
    "\n",
    "        #print('Loading BERT tokenizer')\n",
    "        print(f'PRETRAINED:{self.hparams.pretrained}')\n",
    "\n",
    "        A = AutoTokenizer\n",
    "        self.tokenizer = A.from_pretrained(self.hparams.pretrained, use_fast=True)\n",
    "\n",
    "        print('Tokenizer:', type(self.tokenizer))\n",
    "        \n",
    "        self.datacollator=DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm_probability=0.15)\n",
    "\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        dataset=getWiki(DIR)\n",
    "        self.wikidataset = WikiDataset(self.tokenizer,self.datacollator, dataset)        \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        train_sampler = RandomSampler(self.wikidataset)\n",
    "        \n",
    "        return DataLoader(self.wikidataset,\n",
    "                         #sampler=train_sampler, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=min(NUM_PROCESSORS,self.batch_size,4)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printModelParams(model):\n",
    "    print (model)\n",
    "    # Get all of the model's parameters as a list of tuples.\n",
    "    params = list(model.named_parameters())\n",
    "    print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "    for p in params[-5:]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_value(model):\n",
    "    params = list(model.named_parameters())\n",
    "    print (params[-1][0],params[-1][1][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Configuration to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--pretrained', type=str, default=\"bert-base-uncased\")\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--nr_frozen_epochs', type=int, default=5)\n",
    "    parser.add_argument('--training_portion', type=float, default=0.9)\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "    parser.add_argument('--auto_batch', type=int, default=-1)\n",
    "    parser.add_argument('--learning_rate', type=float, default=2e-5)\n",
    "    parser.add_argument('--frac', type=float, default=1)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--nodes', type=int, default=1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--refresh_rate', type=int, default=1)\n",
    "    parser.add_argument('--check', type=bool, default=False)\n",
    "    parser.add_argument('--rand_dataset', type=str, default=\"dummy\", choices=['temporal','random','dummy'])\n",
    "    \n",
    "    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "\n",
    "    # parser = Model.add_model_specific_args(parser) parser = Data.add_model_specific_args(parser)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(\"BATCH SIZE: \", args.batch_size)\n",
    "    \n",
    "    # start : get training steps\n",
    "    dataProcessor = DataProcessing(args)\n",
    "    dataProcessor.setup()\n",
    "    \n",
    "    args.num_training_steps = len(dataProcessor.train_dataloader())*args.epochs\n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    gpus=-1\n",
    "    if NUM_GPUS>0:\n",
    "        gpus=args.num_gpus        \n",
    "    else:\n",
    "        args.parallel_mode=None\n",
    "        gpus=None\n",
    "    \n",
    "    print(\"USING GPUS:\", gpus)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='loss_epoch',\n",
    "        dirpath=MODEL_SAVE_DIR,\n",
    "        filename=\"V2W-\"+args.pretrained+'-'+str(args.parallel_mode),\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "        save_weights_only=True,\n",
    "        #prefix=\"V2W-\"+args.pretrained+'-'+str(args.parallel_mode),\n",
    "        save_last=True,\n",
    "    )\n",
    "    \n",
    "    if args.check==False:\n",
    "        args.checkpoint_callback = False\n",
    "    elif args.parallel_mode=='dp':\n",
    "        args.callbacks=[checkpoint_callback]        \n",
    "    else:\n",
    "        args.checkpoint_callback = False\n",
    "    \n",
    "    trainer = pl.Trainer.from_argparse_args(args, \n",
    "                                            gpus=gpus,\n",
    "                                            num_nodes=args.nodes, \n",
    "                                            accelerator=args.parallel_mode,\n",
    "                                            max_epochs=args.epochs, \n",
    "                                            gradient_clip_val=1.0,                                            \n",
    "                                            logger=False,\n",
    "                                            progress_bar_refresh_rate=args.refresh_rate,\n",
    "                                            profiler='simple', #'simple',\n",
    "                                            default_root_dir=MODEL_SAVE_DIR,                                            \n",
    "                                            deterministic=True,\n",
    "                                           )\n",
    "\n",
    "    return trainer, dataProcessor, args, dict_args\n",
    "\n",
    "# trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "# next(iter(dataProcessor.test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():    \n",
    "    trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "    \n",
    "    model = Model(**dict_args)    \n",
    "    \n",
    "#     printModelParams(model)\n",
    "#     args.early_stop_callback = EarlyStopping('val_loss')\n",
    "\n",
    "    print(\"Original weights: \");print_model_value(model)\n",
    "    \n",
    "    t0=time.time()\n",
    "    trainer.fit(model, dataProcessor)\n",
    "    print('Training took: ',time.time()-t0)\n",
    "    \n",
    "    print(\"Trained weights: \");print_model_value(model)\n",
    "    \n",
    "#     if args.parallel_mode!='dp':    \n",
    "#         print(\"Saving the last model\")\n",
    "#         #MODEL_NAME=MODEL_SAVE_DIR+\"V2W-\"+args.pretrained+'-'+args.parallel_mode+\".ckpt\"\n",
    "#         MODEL_NAME=MODEL_SAVE_DIR+\"V2W-\"+args.pretrained+\".ckpt\"\n",
    "#         trainer.save_checkpoint(MODEL_NAME)\n",
    "\n",
    "#     print(\"Testing:....\")\n",
    "#     trainer.test(model, dataProcessor.test_dataloader())\n",
    "    \n",
    "    print(\"Training Phase Complete......\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "    \n",
    "    #MODEL_NAME=MODEL_SAVE_DIR+\"V2W-\"+args.pretrained+'-'+args.parallel_mode+\".ckpt\"    \n",
    "    MODEL_NAME=MODEL_SAVE_DIR+\"V2W-\"+args.pretrained+\".ckpt\"    \n",
    "    if args.parallel_mode=='dp':\n",
    "        MODEL_NAME=MODEL_SAVE_DIR+\"V2W-\"+args.pretrained+'-'+args.parallel_mode+\"-last.ckpt\"\n",
    "    \n",
    "    if os.path.exists(MODEL_NAME): \n",
    "        print('Loading Saved Model: ',MODEL_NAME)        \n",
    "    else: \n",
    "        print(\"File not found: \",MODEL_NAME)\n",
    "        return\n",
    "    \n",
    "    model=None\n",
    "    \n",
    "    if args.parallel_mode!='dp':\n",
    "        model = Model.load_from_checkpoint(MODEL_NAME)\n",
    "    else:\n",
    "        model = Model(**dict_args)\n",
    "        print(\"Original weights: \");print_model_value(model)\n",
    "        checkpoint = torch.load(MODEL_NAME, map_location=lambda storage, loc: storage)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    print(\"Loaded weights: \");print_model_value(model)    \n",
    "    trainer.test(model, dataProcessor.test_dataloader())    \n",
    "    print(\"Test Complete......\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "BATCH SIZE:  16\n",
      "PRETRAINED:bert-base-uncased\n",
      "Tokenizer: <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikipedia (/scratch/gilbreth/das90/Dataset/wikipedia/20200501.en/1.0.0/2fe8db1405aef67dff9fcc51e133e1f9c5b0106f9d9e9638188176d278fd5ff1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0920adf6644f83861eaedda67c0e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/das90/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: UserWarning: Checkpoint directory /scratch/gilbreth/das90/Dataset/NVD/Model/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING GPUS: -1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  <class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n",
      "Original weights: \n",
      "model.cls.predictions.transform.LayerNorm.bias tensor([-0.3918,  0.2640,  0.1621,  0.3075], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | BertForMaskedLM | 109 M \n",
      "------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "438.057   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7fd04bdd8d4ba891f48c579e94fba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  86.189         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  82.035         \t|1              \t|  82.035         \t|  95.181         \t|\n",
      "run_training_batch                 \t|  0.64882        \t|125            \t|  81.102         \t|  94.098         \t|\n",
      "optimizer_step_and_closure_0       \t|  0.6469         \t|125            \t|  80.862         \t|  93.82          \t|\n",
      "training_step_and_backward         \t|  0.63049        \t|125            \t|  78.812         \t|  91.441         \t|\n",
      "backward                           \t|  0.57071        \t|125            \t|  71.339         \t|  82.77          \t|\n",
      "model_forward                      \t|  0.057241       \t|125            \t|  7.1552         \t|  8.3018         \t|\n",
      "training_step                      \t|  0.056768       \t|125            \t|  7.096          \t|  8.2331         \t|\n",
      "get_train_batch                    \t|  0.0043722      \t|125            \t|  0.54652        \t|  0.6341         \t|\n",
      "on_train_batch_end                 \t|  0.0021656      \t|125            \t|  0.2707         \t|  0.31408        \t|\n",
      "cache_result                       \t|  5.0702e-05     \t|508            \t|  0.025757       \t|  0.029884       \t|\n",
      "on_train_start                     \t|  0.025744       \t|1              \t|  0.025744       \t|  0.029869       \t|\n",
      "training_step_end                  \t|  7.6335e-05     \t|125            \t|  0.0095419      \t|  0.011071       \t|\n",
      "on_after_backward                  \t|  3.8518e-05     \t|125            \t|  0.0048148      \t|  0.0055863      \t|\n",
      "on_train_epoch_start               \t|  0.0026518      \t|1              \t|  0.0026518      \t|  0.0030767      \t|\n",
      "on_batch_start                     \t|  2.0881e-05     \t|125            \t|  0.0026102      \t|  0.0030284      \t|\n",
      "on_batch_end                       \t|  1.6931e-05     \t|125            \t|  0.0021164      \t|  0.0024555      \t|\n",
      "on_train_batch_start               \t|  1.5689e-05     \t|125            \t|  0.0019611      \t|  0.0022753      \t|\n",
      "on_before_zero_grad                \t|  1.5648e-05     \t|125            \t|  0.001956       \t|  0.0022695      \t|\n",
      "on_train_end                       \t|  0.00051376     \t|1              \t|  0.00051376     \t|  0.00059609     \t|\n",
      "on_train_epoch_end                 \t|  9.0501e-05     \t|1              \t|  9.0501e-05     \t|  0.000105       \t|\n",
      "on_fit_start                       \t|  5.1852e-05     \t|1              \t|  5.1852e-05     \t|  6.0161e-05     \t|\n",
      "on_epoch_start                     \t|  2.101e-05      \t|1              \t|  2.101e-05      \t|  2.4376e-05     \t|\n",
      "on_epoch_end                       \t|  1.6562e-05     \t|1              \t|  1.6562e-05     \t|  1.9216e-05     \t|\n",
      "on_train_dataloader                \t|  1.6113e-05     \t|1              \t|  1.6113e-05     \t|  1.8695e-05     \t|\n",
      "on_before_accelerator_backend_setup\t|  1.3768e-05     \t|1              \t|  1.3768e-05     \t|  1.5974e-05     \t|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took:  85.21399545669556\n",
      "Trained weights: \n",
      "model.cls.predictions.transform.LayerNorm.bias tensor([-0.3917,  0.2638,  0.1623,  0.3074], grad_fn=<SliceBackward>)\n",
      "Training Phase Complete......\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_model()\n",
    "    #test_model()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My py38cu11 Kernel)",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
