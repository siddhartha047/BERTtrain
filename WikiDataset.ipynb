{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of All dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" ##I will find a way to fix this later :(\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" ##I will find a way to fix this later :(\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import RandomSampler,SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoConfig\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWiki(DIR):\n",
    "    \n",
    "    cache_dir=DIR\n",
    "    \n",
    "    dataset=load_dataset('wikipedia', '20200501.en', beam_runner='DirectRunner', cache_dir=cache_dir)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \n",
    "    if os.uname()[1].find('gilbreth')==0: ##if not darwin(mac/locallaptop)\n",
    "        DIR='/scratch/gilbreth/das90/Dataset'\n",
    "    elif os.uname()[1].find('unimodular')==0:\n",
    "        DIR='/scratch2/das90/Dataset'\n",
    "    elif os.uname()[1].find('Siddharthas')==0:\n",
    "        DIR='/Users/siddharthashankardas/Purdue/Dataset'  \n",
    "    else:\n",
    "        DIR='./Results'\n",
    "        \n",
    "    cache_dir=DIR\n",
    "    \n",
    "    dataset=getWiki(cache_dir)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f91bf6342a4ee19e4ef743e5091261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc86ca1eaf7244da927e917f290d1fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Old caching folder /scratch/gilbreth/das90/Dataset/wikipedia/20200501.en/1.0.0/2fe8db1405aef67dff9fcc51e133e1f9c5b0106f9d9e9638188176d278fd5ff1 for dataset wikipedia exists but not data were found. Removing it. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikipedia/20200501.en (download: 16.99 GiB, generated: 17.07 GiB, post-processed: Unknown size, total: 34.06 GiB) to /scratch/gilbreth/das90/Dataset/wikipedia/20200501.en/1.0.0/2fe8db1405aef67dff9fcc51e133e1f9c5b0106f9d9e9638188176d278fd5ff1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60eff0db8b548c28ba568efaf5f0e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b25558a3adc4ed597268055e053e0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/18.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikipedia downloaded and prepared to /scratch/gilbreth/das90/Dataset/wikipedia/20200501.en/1.0.0/2fe8db1405aef67dff9fcc51e133e1f9c5b0106f9d9e9638188176d278fd5ff1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b950f634fe894e1d994c5a8953cb3ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset=test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6078422, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'text'],\n",
       "    num_rows: 6078422\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2073,  2003,   103,  1029,  1006,  1007,  2003,  1037,  4051,\n",
       "          3059,  4038,  1011,  3689, 28026,  2856,  2011, 10704,  5811, 13348,\n",
       "          3490,  1012,  1996,  2143,  2018,  1037, 11587,  2537,  2138,  1010,\n",
       "          2044,  5008,  2070,  5019,  1010,  5811, 13348,  3490,  2439,  3037,\n",
       "          1999,  1996,  2143,  1998,  4704,  1996,  2275,  1012,  1996,  2147,\n",
       "          2001,  2949,  2044,   103,  1037,  2095,  1010,  3701,   103,  7986,\n",
       "         12256,  6610,   103,   103,  2007,  2070,  5019,  2036,  2915,  2011,\n",
       "         12776,  3695, 11865, 15472,  2072,  1998, 20493,   103,  5498,   103,\n",
       "          2750,  2008,  1010,  5811, 27843,  3490,  2003,  1996,  7082,  5827,\n",
       "          2472,  1997,  1996,  2143,  1012,  5436,  8190,  1998, 13460,  1997,\n",
       "          2019,  4654,  1011, 20462,  1012,  7861, 16313, 14050,  1998,  4487,\n",
       "         27572, 24117,  2098,  2011,  2166,   103,  2002,  2097,  2574,  2933,\n",
       "          2010,   103,  2000,  3827,  1012,  3459,  2000,  3406,  1024, 17485,\n",
       "          8840, 14855,  8663,  2080, 12297,  9587, 19666,  2906,  1024, 11166,\n",
       "          3981,  9152,  2696, 13985,   103, 13955,  5524,  2696,  4487,  4907,\n",
       "          4143,   103,  3540,   103,  6392,  5498,   103,  3814, 12752,  2080,\n",
       "         25199,   103, 11113,  6444,  2080, 11939,  3630,  4980, 19332,  6494,\n",
       "          1024, 11941, 27210,  7174, 16183,  6767,  5622, 22083,  5332,  1024,\n",
       "         11941, 27210,  7174,  1001,  1016,   103, 22264, 10672,  4571,  1024,\n",
       "          2482, 19357,  3406,   103,  3995,  1040,  1005, 15669, 18719,  2080,\n",
       "          1024, 21025,   103,  3401,   103,   103, 19036,  2072,  1024,  9047,\n",
       "         16558, 11261,  2704,  2080, 24712,   103,  8486,   103,  1024, 20704,\n",
       "          6767, 11266,   103,  4487, 18940, 21748,   103, 15541, 19594,  3490,\n",
       "          1024, 15541,  7604,  6327,  6971,  4696,  1024,  4051, 26267,  4696,\n",
       "          1024,  3059,  4038,  1011,  3689,  3152,  4696,   103,  4856,  4038,\n",
       "          1011,  3689,  3152,  4696,  1024,  3152,  2856,  2011, 10704,  5811,\n",
       "         13348,  3490,  4696,  1024,  4012, 16969,  2035,  1005, 28059,  4696,\n",
       "           103,   103,  2275,  1999,  4199,  4696,  1024,   103,  3152,  4696,\n",
       "          1024,  3152,  2550,  2011,   103,  2139,   103,  6137,  2015,  4696,\n",
       "          1024,  3152,  2550,  2011,  9758, 21179,  2072,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([ -100,  -100,  -100,  4071,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  2143,  -100,  2011,  -100,  -100,  -100,\n",
       "          3490,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  1012,  -100,  -100,\n",
       "          -100,  -100,  -100,  2055,  -100,  -100,  -100,  -100,  2013,  -100,\n",
       "          -100,  -100,  6894,  1010,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  3062,  -100,  1012,\n",
       "          -100,  -100,  -100,  -100, 13348,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  1010,  -100,  -100,  -100,  -100,\n",
       "          -100,  2709,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  1024,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100, 23151,  -100,  6904,  -100,  -100,  1024,  -100,  -100,  -100,\n",
       "          -100,  1024,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100, 22873,  6902,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  1057,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100, 21041,  -100,  7986, 14542,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100, 21368,  -100,  3597,  -100,  -100,\n",
       "          -100,  -100,  2080,  -100,  -100,  -100,  2063,  -100,  9758,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  3152,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  1024,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          1024,  3152,  -100,  -100,  -100,  -100,  -100,  3059,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100, 22412,  -100, 14718,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(examples):\n",
    "    return berttokenizer(examples['text'], truncation=True)\n",
    "\n",
    "class WikiDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, berttokenizer, collator, dataset):\n",
    "        self.tokenizer = berttokenizer\n",
    "        self.dataset = dataset\n",
    "        self.collator = collator\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = encode(dataset['train'][idx])        \n",
    "        item = self.collator([item])\n",
    "        item = {key: val[0] for key, val in item.items()}\n",
    "    \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return dataset.num_rows\n",
    "        \n",
    "A=AutoTokenizer\n",
    "berttokenizer=A.from_pretrained('bert-base-uncased')\n",
    "datacollator=DataCollatorForLanguageModeling(tokenizer=berttokenizer,mlm_probability=0.15, mlm=True)\n",
    "\n",
    "wikidataset = WikiDataset(berttokenizer,datacollator, dataset)\n",
    "\n",
    "wikidataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6078422"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My py38cu11 Kernel)",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
